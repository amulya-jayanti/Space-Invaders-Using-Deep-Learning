{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f111c7-0350-41b6-a32a-430a783660bc",
   "metadata": {},
   "source": [
    "**Step-0: Install dependencies** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45092f3e-eb68-46c0-885e-7baa2c8c3921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: keras in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (3.9.0)\n",
      "Requirement already satisfied: keras-rl2 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (1.0.5)\n",
      "Requirement already satisfied: gym in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from keras) (0.14.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from rich->keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: gymnasium in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from gymnasium) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: ale-py in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (0.10.2)\n",
      "Requirement already satisfied: numpy>1.20 in /opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages (from ale-py) (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow keras keras-rl2 gym\n",
    "!pip install gymnasium\n",
    "!pip install ale-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e3778f7-eb7a-4448-b893-f0323b6b4090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rl_final_env/bin/python\n",
      "Python 3.11.11\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd026e1-08d0-481a-951e-94bf2b95f1cb",
   "metadata": {},
   "source": [
    "**Step 1: Test Random Environment with OpenAI Gym**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c5f50e-5710-47d4-a4e5-c45386bee1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be5c7470-bacc-4023-9301-5a9d6817d4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"SpaceInvaders-v4\",render_mode=\"human\")\n",
    "# SpaceInvaders-v5 environment returns an image as part of the state. \n",
    "# We extract the shape of image to pass to structure our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77eaef5b-d0fe-4408-9640-320575260b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels=env.observation_space.shape\n",
    "actions=env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f46870-5a4d-4748-9316-07a2f2a1c683",
   "metadata": {},
   "source": [
    "**We can see the actions in https://www.gymlibrary.dev/environments/atari/space_invaders/, however to see them here, lets unwrap the environment and check what actions we have in action_space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd382cb-693a-4b5f-9ebe-16681f92f751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2983ae-1e43-4bf2-adc4-af4f50889481",
   "metadata": {},
   "source": [
    "**So within our environment, our agent can take the following actions: No operation, Fire, Move right, Move left, Fire right and Fire Left**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a034a1a-7f3a-4d4a-96a8-646ff9a82c36",
   "metadata": {},
   "source": [
    "For the random test, we will play 5 episodes(5 games of Space Invaders)\n",
    "\n",
    "Let's carefully understand what we did here.\n",
    "\n",
    "1. env.reset() sets our state to reset.\n",
    "2. done=False is initial setting so that we play the game until we are done, which means, either until we achieve high score or we die in the game.\n",
    "3. Score is a counter to keep track fo our game score.\n",
    "4. random.choice is to make a choice among the 6 actions we have above. This will help us understand how our agent randomly performs taking these actions in the game.\n",
    "5. env.step(action) allows us to take an action we chose randomly earlier, and apply it to our environment, after which we take the following info:\n",
    "   \n",
    "    **n_state:** The new state \n",
    "\n",
    "    **reward:** The reward for the action \n",
    "\n",
    "    **done:** Whether the episode ended \n",
    "\n",
    "    **truncated:** Whether the episode was forcefully stopped (time limit, etc.) \n",
    "\n",
    "    **info:** Additional environment details\n",
    "\n",
    "7. reward of taking an action is constantly added to our score, which maintains the running total through the course of the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adae309d-1a09-46e7-8e49-c3b227076266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 14:09:30.818 python[20873:875371] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-07 14:09:30.818 python[20873:875371] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:120.0\n",
      "Episode:2 Score:265.0\n",
      "Episode:3 Score:260.0\n",
      "Episode:4 Score:75.0\n",
      "Episode:5 Score:30.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1,2,3,4,5])\n",
    "        n_state, reward, done, info, _ = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c124a753-25c5-41ed-97fc-ca3913b722a7",
   "metadata": {},
   "source": [
    "**This performed fairly well, however we can see that the scores are not consistent, with some high and some low, so our end goal is to train our model in a way that the agent performs to get consistent high rewards as it learns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422dcb07-8b1c-41f5-b1e1-7083c756b296",
   "metadata": {},
   "source": [
    "**Step-2: Create a Deep Learning Model with Stable Baselines3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf34daf5-2712-4445-a36d-8ff7447db060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "env = DummyVecEnv([lambda: env]) #Wraps the environment for Stable-Baselines3, which requires vectorized environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eb66ee5-7c5c-44f4-8ad8-806d22a58cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 18   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 109  |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 231         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007818995 |\n",
      "|    clip_fraction        | 0.0362      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | 0.00237     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.13        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    value_loss           | 11.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 352         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009881314 |\n",
      "|    clip_fraction        | 0.0855      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.4         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 11.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 473         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008849178 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.0177      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 251         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 143         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 573         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010862192 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.77       |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.7         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 12.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rl_final_env/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Policy - Mean Reward: 317.33\n"
     ]
    }
   ],
   "source": [
    "mlp_model = PPO(\"MlpPolicy\", env, verbose=2)\n",
    "mlp_model.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluate MLP Policy\n",
    "mlp_mean_reward, _ = evaluate_policy(mlp_model, env, n_eval_episodes=15)\n",
    "mlp_model.save(\"SpaceInvaders_MLP\")\n",
    "\n",
    "print(f\"MLP Policy - Mean Reward: {mlp_mean_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6717a2a6-aef2-4f31-b621-a692af62273d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 18   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 109  |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 270         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012101004 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.00262     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.23        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00217    |\n",
      "|    value_loss           | 11.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 432         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014601601 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | -0.00396    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.0195      |\n",
      "|    value_loss           | 99.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 596         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016969103 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.75       |\n",
      "|    explained_variance   | -0.0461     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.51        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.00446     |\n",
      "|    value_loss           | 14.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 757        |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01892546 |\n",
      "|    clip_fraction        | 0.198      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.74      |\n",
      "|    explained_variance   | 0.702      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.03       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.00278   |\n",
      "|    value_loss           | 9.96       |\n",
      "----------------------------------------\n",
      "CNN Policy - Mean Reward: 117.00\n"
     ]
    }
   ],
   "source": [
    "# Train model using CNN policy\n",
    "cnn_model = PPO(\"CnnPolicy\", env, verbose=2)\n",
    "cnn_model.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluate CNN Policy\n",
    "cnn_mean_reward, _ = evaluate_policy(cnn_model, env, n_eval_episodes=15)\n",
    "cnn_model.save(\"SpaceInvaders_CNN\")\n",
    "\n",
    "print(f\"CNN Policy - Mean Reward: {cnn_mean_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0b6d4-4572-4bbe-b4c2-e00cfa4fe70e",
   "metadata": {},
   "source": [
    "- It’s surprising that MLP Policy performed better than CNN Policy for Space Invaders, but let’s analyze why this might be happening.\n",
    "\n",
    "- 10,000 timesteps might not be enough for CNN to outperform MLP. CNNs require more training to extract spatial features effectively.\n",
    "\n",
    "- So now, let's try with number of timesteps=50,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10c59581-22c0-4e39-b6d3-295e04b23666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 18   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 111  |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 272         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010836471 |\n",
      "|    clip_fraction        | 0.0845      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | -0.00298    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.773       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0027     |\n",
      "|    value_loss           | 8.09        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 411          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0135919945 |\n",
      "|    clip_fraction        | 0.151        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.76        |\n",
      "|    explained_variance   | 0.183        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.29         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0109      |\n",
      "|    value_loss           | 10.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 546         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015778804 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.336       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.97        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 13          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 683         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025401674 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.77       |\n",
      "|    explained_variance   | 0.0616      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.015       |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 821         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013988059 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.77       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.31        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.0032      |\n",
      "|    value_loss           | 15.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 963        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02028228 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.76      |\n",
      "|    explained_variance   | 0.12       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.76       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | 0.00324    |\n",
      "|    value_loss           | 15.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 1101        |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019233335 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.79        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00263    |\n",
      "|    value_loss           | 8.68        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 1240        |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016920822 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.74       |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.23        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    value_loss           | 9.29        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 1380       |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01836032 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.68      |\n",
      "|    explained_variance   | 0.721      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.81       |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0014    |\n",
      "|    value_loss           | 10.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 1520        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013011611 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.574       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.8         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.00591     |\n",
      "|    value_loss           | 12.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 1659        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016282158 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.86        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.00291     |\n",
      "|    value_loss           | 13.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 1800        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018738985 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.77        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0016     |\n",
      "|    value_loss           | 8.57        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 1940        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022882389 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.62       |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.34        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.00329     |\n",
      "|    value_loss           | 13.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 2079       |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04048962 |\n",
      "|    clip_fraction        | 0.288      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.66      |\n",
      "|    explained_variance   | 0.15       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.74       |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | 0.0229     |\n",
      "|    value_loss           | 51.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 2219        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026015466 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.4         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    value_loss           | 38.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 2359       |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03992471 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.58      |\n",
      "|    explained_variance   | 0.175      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.25       |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0074    |\n",
      "|    value_loss           | 17.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 2499        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058779802 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.426       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.47        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.0021      |\n",
      "|    value_loss           | 16.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 2638        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030939674 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.74        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.0115      |\n",
      "|    value_loss           | 10.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 2774       |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02635809 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.65      |\n",
      "|    explained_variance   | 0.529      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.25       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | 0.00305    |\n",
      "|    value_loss           | 10.6       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 2914       |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01639779 |\n",
      "|    clip_fraction        | 0.258      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.64      |\n",
      "|    explained_variance   | 0.671      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.68       |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.00377   |\n",
      "|    value_loss           | 7.26       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 3053        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022562508 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.34        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.00189     |\n",
      "|    value_loss           | 14.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 3194       |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01865205 |\n",
      "|    clip_fraction        | 0.237      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.57      |\n",
      "|    explained_variance   | 0.639      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.87       |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | 0.00237    |\n",
      "|    value_loss           | 12.2       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 3336       |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03974403 |\n",
      "|    clip_fraction        | 0.321      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.48      |\n",
      "|    explained_variance   | 0.145      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.48       |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | 0.0179     |\n",
      "|    value_loss           | 55.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 3477        |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024189759 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.38        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.00571     |\n",
      "|    value_loss           | 16.6        |\n",
      "-----------------------------------------\n",
      "CNN Policy - Mean Reward: 291.33\n"
     ]
    }
   ],
   "source": [
    "# Train model using CNN policy\n",
    "cnn_model = PPO(\"CnnPolicy\", env, verbose=2)\n",
    "cnn_model.learn(total_timesteps=50000)\n",
    "\n",
    "# Evaluate CNN Policy\n",
    "cnn_mean_reward, _ = evaluate_policy(cnn_model, env, n_eval_episodes=15)\n",
    "cnn_model.save(\"SpaceInvaders_CNN\")\n",
    "\n",
    "print(f\"CNN Policy - Mean Reward: {cnn_mean_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d30a04c7-b150-42eb-adf7-a6cb30820a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 31   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 64   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 146         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008289233 |\n",
      "|    clip_fraction        | 0.0787      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | -0.0028     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.23        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 12.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 26           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 228          |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063991845 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.78        |\n",
      "|    explained_variance   | 0.0163       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.69         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00946     |\n",
      "|    value_loss           | 156          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 310         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010281038 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.77       |\n",
      "|    explained_variance   | 0.0642      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.49        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 143         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 390         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014349755 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.74       |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.57        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 15.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 472         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010210098 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.75       |\n",
      "|    explained_variance   | 0.574       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.98        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 17.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 25         |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 554        |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01065097 |\n",
      "|    clip_fraction        | 0.0905     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.74      |\n",
      "|    explained_variance   | 0.659      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.87       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    value_loss           | 16.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 632         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012647758 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.17        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 13.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 711         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013575712 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.74       |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.1         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 14.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 789         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010275453 |\n",
      "|    clip_fraction        | 0.0987      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.15        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 170         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 867         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013246315 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.5         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 15          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 945         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014229339 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.639       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.95        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 21.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 1023        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016366191 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.69       |\n",
      "|    explained_variance   | 0.561       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.96        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 16.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 1101        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012921185 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.69       |\n",
      "|    explained_variance   | 0.577       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.54        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 1179        |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015443476 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.21        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 17.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 1256        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016962778 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.68       |\n",
      "|    explained_variance   | 0.654       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.29        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 14.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 1333        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015878718 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.69       |\n",
      "|    explained_variance   | 0.676       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.66        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 16.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 1410        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011370437 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.68        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 17.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 1489        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013907634 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.69       |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.19        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 18.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 1570        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010648266 |\n",
      "|    clip_fraction        | 0.0928      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.2        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 1651        |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010699099 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.69        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 17.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 1731        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013795845 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.68       |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.41        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 12.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 1808        |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012662612 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.09        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 17.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 1886        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010734839 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.19        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 125         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 1964        |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010494607 |\n",
      "|    clip_fraction        | 0.0997      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 86.1        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 98.3        |\n",
      "-----------------------------------------\n",
      "MLP Policy - Mean Reward: 287.00\n"
     ]
    }
   ],
   "source": [
    "mlp_model = PPO(\"MlpPolicy\", env, verbose=2)\n",
    "mlp_model.learn(total_timesteps=50000)\n",
    "\n",
    "# Evaluate MLP Policy\n",
    "mlp_mean_reward, _ = evaluate_policy(mlp_model, env, n_eval_episodes=15)\n",
    "mlp_model.save(\"SpaceInvaders_MLP\")\n",
    "\n",
    "print(f\"MLP Policy - Mean Reward: {mlp_mean_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb5a85-abdf-4e3c-8cdf-a1c68bbc6b75",
   "metadata": {},
   "source": [
    "**Variations:**\n",
    "\n",
    "We first use MlpPolicy, but we can see the mean reward isn't as high as that for the lower number of steps(10,000), \"CnnPolicy\" has better reward. We will understand why. But let's first see what happens in our code below:\n",
    "\n",
    "- \"CnnPolicy\" refers to a Convolutional Neural Network (CNN) policy, which is best for image-based environments\n",
    "- Learning_rate=1e-4 sets the learning rate for policy optimization.\n",
    "- n_steps=2048 are the number of steps before updating the model.\n",
    "- learn(total_timesteps=50000) trains the agent for 50,000 steps.\n",
    "- saves the trained model for later use.\n",
    "\n",
    "Explaining difference between performance of MlpPolicy and CnnPolicy:\n",
    "\n",
    "- MlpPolicy works well for low-dimensional, vector-based observations (e.g., stock prices, sensor readings, or tabular data).\n",
    "- Hence, it is more suitable for environments where observations are represented as numerical arrays rather than images for example: Space Invaders provides raw pixel images as observations, so it cannot perceive it well. \n",
    "- Also, MLP (Multi-Layer Perceptron) does not process spatial features well.\n",
    "- Over 50,000 steps, MLP policy has mean reward of 287 while CNN Policy has 291.33.\n",
    "\n",
    "**Fun fact:** Without convolutional layers, the model cannot detect enemy positions, player location, or bullet trajectories properly. CnnPolicy is proven to work well on Atari games, as seen in DeepMind’s DQN paper.\n",
    "- So we consider CnnPolicy to extract spatial features from images and detect patterns like enemies, bullets, and player movement effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0d7cb-b3b7-4db0-8b7c-fd660d44ed11",
   "metadata": {},
   "source": [
    "**Step-3: Test the model performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b2e2c6d-ee0e-46c2-9ff5-62c1265555ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - Score: [380.]\n",
      "Episode 2 - Score: [500.]\n",
      "Episode 3 - Score: [170.]\n",
      "Episode 4 - Score: [220.]\n",
      "Episode 5 - Score: [185.]\n",
      "Episode 6 - Score: [175.]\n",
      "Episode 7 - Score: [460.]\n",
      "Episode 8 - Score: [165.]\n",
      "Episode 9 - Score: [80.]\n",
      "Episode 10 - Score: [240.]\n",
      "Mean Score across 10 episodes: [257.5]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "def test_model(env, episodes=10, PATH_SAVED=\"SpaceInvaders_CNN\"):\n",
    "    # Wrap the environment correctly (without VecFrameStack)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecTransposeImage(env)  # Ensures correct shape for CNN input\n",
    "\n",
    "    # Load the saved model\n",
    "    model = PPO.load(PATH_SAVED, env=env)\n",
    "\n",
    "    # Reset the environment and get the first observation\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Get frame size for saving video\n",
    "    frame = env.render(mode=\"rgb_array\")\n",
    "    frame_size = (frame.shape[1], frame.shape[0])  \n",
    "    output_file = f'{PATH_SAVED}-video.mp4'  \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  \n",
    "    fps = 30.0  \n",
    "    video_writer = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "    \n",
    "    # Loop through episodes\n",
    "    total_score = 0\n",
    "    for i in range(episodes):\n",
    "        obs = env.reset()  # Reset environment for each episode\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            frame = env.render(mode=\"rgb_array\")  # Get frame for video\n",
    "            action, _ = model.predict(obs)  # Get action from the model\n",
    "            obs, reward, done, _ = env.step(action)  # Take action in environment\n",
    "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert frame to BGR (for OpenCV)\n",
    "            video_writer.write(frame_bgr)  # Write frame to video\n",
    "            score += reward  # Keep track of the score\n",
    "        \n",
    "        # Display score for each episode\n",
    "        print(f'Episode {i + 1} - Score: {score}')\n",
    "        total_score += score\n",
    "    \n",
    "    # Calculate and print the mean score for all episodes\n",
    "    mean_score = total_score / episodes\n",
    "    print(f'Mean Score across {episodes} episodes: {mean_score}')\n",
    "    \n",
    "    # Save the model video\n",
    "    video_writer.release()\n",
    "    env.close()\n",
    "\n",
    "# Create and wrap the environment properly\n",
    "env = gym.make('SpaceInvaders-v4', render_mode='rgb_array')\n",
    "\n",
    "# Test the CNN policy\n",
    "cnn_mean_score = test_model(env, episodes=10, PATH_SAVED='SpaceInvaders_CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10afd3e-b97b-4c31-ac1c-a1adf5dac478",
   "metadata": {},
   "source": [
    "CNN Policy Model gave us mean reward of 257.5 across the 10 episodes. Let's also for reference check how MLP Policy performed, as their training scores weren't very different. Maybe if converged more with high training steps like 100000 or more, it will show a clear distinction between their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b4f7da4-1d33-4252-b581-fe2e5763f173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - Score: [90.]\n",
      "Episode 2 - Score: [70.]\n",
      "Episode 3 - Score: [210.]\n",
      "Episode 4 - Score: [75.]\n",
      "Episode 5 - Score: [135.]\n",
      "Episode 6 - Score: [110.]\n",
      "Episode 7 - Score: [210.]\n",
      "Episode 8 - Score: [110.]\n",
      "Episode 9 - Score: [135.]\n",
      "Episode 10 - Score: [210.]\n",
      "Mean Score across 10 episodes: [135.5]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "def test_model(env, episodes=10, PATH_SAVED=\"SpaceInvaders_MLP\"):\n",
    "    # Wrap the environment correctly (without VecFrameStack)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecTransposeImage(env)  # Ensures correct shape for CNN input\n",
    "\n",
    "    # Load the saved model\n",
    "    model = PPO.load(PATH_SAVED, env=env)\n",
    "\n",
    "    # Reset the environment and get the first observation\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Get frame size for saving video\n",
    "    frame = env.render(mode=\"rgb_array\")\n",
    "    frame_size = (frame.shape[1], frame.shape[0])  \n",
    "    output_file = f'{PATH_SAVED}-video.mp4'  \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  \n",
    "    fps = 30.0  \n",
    "    video_writer = cv2.VideoWriter(output_file, fourcc, fps, frame_size)\n",
    "    \n",
    "    # Loop through episodes\n",
    "    total_score = 0\n",
    "    for i in range(episodes):\n",
    "        obs = env.reset()  # Reset environment for each episode\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            frame = env.render(mode=\"rgb_array\")  # Get frame for video\n",
    "            action, _ = model.predict(obs)  # Get action from the model\n",
    "            obs, reward, done, _ = env.step(action)  # Take action in environment\n",
    "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert frame to BGR (for OpenCV)\n",
    "            video_writer.write(frame_bgr)  # Write frame to video\n",
    "            score += reward  # Keep track of the score\n",
    "        \n",
    "        # Display score for each episode\n",
    "        print(f'Episode {i + 1} - Score: {score}')\n",
    "        total_score += score\n",
    "    \n",
    "    # Calculate and print the mean score for all episodes\n",
    "    mean_score = total_score / episodes\n",
    "    print(f'Mean Score across {episodes} episodes: {mean_score}')\n",
    "    \n",
    "    # Save the model video\n",
    "    video_writer.release()\n",
    "    env.close()\n",
    "\n",
    "# Create and wrap the environment properly\n",
    "env = gym.make('SpaceInvaders-v4', render_mode='rgb_array')\n",
    "\n",
    "# Test the CNN policy\n",
    "cnn_mean_score = test_model(env, episodes=10, PATH_SAVED='SpaceInvaders_MLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fee3e5-d24a-40c2-88b1-d7f123168c69",
   "metadata": {},
   "source": [
    "**Evidently CNN policy has performed better, as we got a mean score of 257.5 over 10 episodes, compared to MLP Policy which gave mean score of only 135.5.**\n",
    "\n",
    "- We have an output mp4 video file to show the game play by our agent trained on variants of MLP and CNN policy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl_final_env)",
   "language": "python",
   "name": "rl_final_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
